{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0966d5a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MountainCar\n",
    "\n",
    "This is our solution for the openAI gym MountainCar-v0 problem.\n",
    "The 2 solutions we used were:\n",
    "\n",
    "    - QLearning \n",
    "    - SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e188fc46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e853d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9fe3fc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "# Makes the observation and action space discrete and places them in bins.\n",
    "pos_space = np.linspace(-1.2, 0.6, 18)    # 18 bins for the position space \n",
    "vel_space = np.linspace(-0.07, 0.07, 28)  # 28 bins for the velocity space\n",
    "\n",
    "# given observation, returns what bin\n",
    "def getState(observation):\n",
    "    pos, vel = observation\n",
    "    pos_bin = np.digitize(pos, pos_space)\n",
    "    vel_bin = np.digitize(vel, vel_space)\n",
    "    return (pos_bin, vel_bin)            \n",
    "\n",
    "# Creates a new empty Q-table for the environment\n",
    "def createEmptyQTable():\n",
    "    states = []\n",
    "    for pos in range(len(pos_space) + 1):\n",
    "        for vel in range(len(vel_space) + 1):\n",
    "            states.append((pos,vel))\n",
    "        \n",
    "    Q = {}\n",
    "    for state in states:\n",
    "        for action in range(env.action_space.n):\n",
    "            Q[state, action] = 0\n",
    "    return Q\n",
    "\n",
    "# Given a state and a set of actions\n",
    "# returns action that has the highest Q-value\n",
    "def maxAction(Q, state, actions=[0, 1, 2]):\n",
    "    values = np.array([Q[state,a] for a in actions])\n",
    "    action = np.argmax(values)\n",
    "    return action\n",
    "\n",
    "# Saves a variable as a file, so we dont have to re-train each time we want to run the simulation\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load a variable from file\n",
    "def load_obj(name ):\n",
    "    print(name + '.pkl')\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        f.seek(0)\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1e2d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db448e42",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def Q_learning(alpha, gamma, epsilon, episodes):\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env._max_episode_steps = 1000\n",
    "\n",
    "    # Create an empty Q-table\n",
    "    Q = createEmptyQTable()\n",
    "\n",
    "    score = 0\n",
    "    reward_list = []\n",
    "    avg_reward_list = []\n",
    "    # Variable to keep track of the total score obtained\n",
    "    # at each episode to plot it later\n",
    "    total_score = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        state = getState(observation)\n",
    "    \n",
    "        if i % 500 == 0:\n",
    "            print(f'episode: {i}, score: {score}, epsilon: {epsilon:0.3f}')\n",
    "    \n",
    "        score = 0\n",
    "        while not done:\n",
    "            # e-Greedy strategy\n",
    "            # Explore random action with probability epsilon\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            # Take best action with probability 1-epsilon\n",
    "            else:\n",
    "                action = maxAction(Q, state)\n",
    "        \n",
    "            # Observe next state based on the next step\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            next_state = getState(next_observation)\n",
    "        \n",
    "            # Add reward to the score of the episode\n",
    "            score += reward\n",
    "        \n",
    "            # Get next action\n",
    "            next_action = maxAction(Q, next_state)\n",
    "        \n",
    "            # Update Q value for state and action given the bellman equation\n",
    "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action]) \n",
    "        \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "\n",
    "        # Save score for this episode\n",
    "        total_score[i] = score\n",
    "    \n",
    "        reward_list.append(score)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "                avg_reward = np.mean(reward_list)\n",
    "                avg_reward_list.append(avg_reward)\n",
    "                reward_list = []\n",
    "            \n",
    "            \n",
    "        # Reduce epsilon \n",
    "        epsilon = epsilon - 2/episodes if epsilon > 0.01 else 0.01\n",
    "        \n",
    "    save_obj(Q, 'trained-QLearning')\n",
    "    return avg_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6548693",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SARSA-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f7a8ca",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def SARSA_learning(alpha, gamma, epsilon, episodes):\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env._max_episode_steps = 1000\n",
    "\n",
    "    # Create an empty Q-table\n",
    "    Q = createEmptyQTable()\n",
    "\n",
    "    score = 0\n",
    "    reward_list = []\n",
    "    avg_reward_list = []\n",
    "    # Variable to keep track of the total score obtained at each episode\n",
    "    total_score = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        if i % 500 == 0:\n",
    "            print(f'episode: {i}, score: {score}, epsilon: {epsilon:0.3f}')\n",
    "    \n",
    "        observation = env.reset()\n",
    "        state = getState(observation)\n",
    "\n",
    "        # e-Greedy strategy\n",
    "        # Explore random action with probability epsilon\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        # Take best action with probability 1-epsilon\n",
    "        else:\n",
    "            action = maxAction(Q, state)\n",
    "    \n",
    "        score = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Take action and observe next state\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            next_state = getState(next_observation)\n",
    "        \n",
    "            # Get next action following e-Greedy policy\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action= maxAction(Q, next_state)\n",
    "        \n",
    "            # Add reward to the score of the episode\n",
    "            score += reward\n",
    "\n",
    "            # Update Q value for state and action given the bellman equation\n",
    "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action]) \n",
    "        \n",
    "            # Move to next state, and next action\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "        total_score[i] = score\n",
    "    \n",
    "        reward_list.append(score)\n",
    "    \n",
    "        if (i+1) % 100 == 0:\n",
    "            avg_reward = np.mean(reward_list)\n",
    "            avg_reward_list.append(avg_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "        epsilon = epsilon - 2/episodes if epsilon > 0.01 else 0.01\n",
    "    save_obj(Q, 'trained-SARSA')    \n",
    "    return avg_reward_list        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bf0b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Run the training\n",
    "\n",
    "Here we run the training with the defined variables as hyperparameters\n",
    "\n",
    "It saves the rewards from each episode and returns an average of every 100 episodes. \n",
    "\n",
    "This was meant for plotting, which we unfortunately didn't have enough time to implement properly, but it also saves the trained data in a pickle file for running.\n",
    "We did however include a few examples of the plotting from the testing phase in our video presentation.\n",
    "Those are from another notebook, with another implementation of the Q-learning algorithm. Sadly not on SARSA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea947da0",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, score: 0, epsilon: 1.000\n",
      "episode: 500, score: -1000.0, epsilon: 0.950\n",
      "episode: 1000, score: -1000.0, epsilon: 0.900\n",
      "episode: 1500, score: -1000.0, epsilon: 0.850\n",
      "episode: 2000, score: -627.0, epsilon: 0.800\n",
      "episode: 2500, score: -741.0, epsilon: 0.750\n",
      "episode: 3000, score: -787.0, epsilon: 0.700\n",
      "episode: 3500, score: -478.0, epsilon: 0.650\n",
      "episode: 4000, score: -292.0, epsilon: 0.600\n",
      "episode: 4500, score: -312.0, epsilon: 0.550\n",
      "episode: 5000, score: -309.0, epsilon: 0.500\n",
      "episode: 5500, score: -325.0, epsilon: 0.450\n",
      "episode: 6000, score: -298.0, epsilon: 0.400\n",
      "episode: 6500, score: -218.0, epsilon: 0.350\n",
      "episode: 7000, score: -237.0, epsilon: 0.300\n",
      "episode: 7500, score: -379.0, epsilon: 0.250\n",
      "episode: 8000, score: -226.0, epsilon: 0.200\n",
      "episode: 8500, score: -196.0, epsilon: 0.150\n",
      "episode: 9000, score: -149.0, epsilon: 0.100\n",
      "episode: 9500, score: -168.0, epsilon: 0.050\n",
      "episode: 10000, score: -149.0, epsilon: 0.010\n",
      "episode: 10500, score: -197.0, epsilon: 0.010\n",
      "episode: 11000, score: -128.0, epsilon: 0.010\n",
      "episode: 11500, score: -117.0, epsilon: 0.010\n",
      "episode: 12000, score: -158.0, epsilon: 0.010\n",
      "episode: 12500, score: -186.0, epsilon: 0.010\n",
      "episode: 13000, score: -158.0, epsilon: 0.010\n",
      "episode: 13500, score: -233.0, epsilon: 0.010\n",
      "episode: 14000, score: -164.0, epsilon: 0.010\n",
      "episode: 14500, score: -161.0, epsilon: 0.010\n",
      "episode: 15000, score: -192.0, epsilon: 0.010\n",
      "episode: 15500, score: -176.0, epsilon: 0.010\n",
      "episode: 16000, score: -154.0, epsilon: 0.010\n",
      "episode: 16500, score: -139.0, epsilon: 0.010\n",
      "episode: 17000, score: -167.0, epsilon: 0.010\n",
      "episode: 17500, score: -149.0, epsilon: 0.010\n",
      "episode: 18000, score: -222.0, epsilon: 0.010\n",
      "episode: 18500, score: -158.0, epsilon: 0.010\n",
      "episode: 19000, score: -143.0, epsilon: 0.010\n",
      "episode: 19500, score: -147.0, epsilon: 0.010\n",
      "episode: 0, score: 0, epsilon: 1.000\n",
      "episode: 500, score: -1000.0, epsilon: 0.950\n",
      "episode: 1000, score: -1000.0, epsilon: 0.900\n",
      "episode: 1500, score: -1000.0, epsilon: 0.850\n",
      "episode: 2000, score: -687.0, epsilon: 0.800\n",
      "episode: 2500, score: -591.0, epsilon: 0.750\n",
      "episode: 3000, score: -1000.0, epsilon: 0.700\n",
      "episode: 3500, score: -322.0, epsilon: 0.650\n",
      "episode: 4000, score: -419.0, epsilon: 0.600\n",
      "episode: 4500, score: -395.0, epsilon: 0.550\n",
      "episode: 5000, score: -256.0, epsilon: 0.500\n",
      "episode: 5500, score: -234.0, epsilon: 0.450\n",
      "episode: 6000, score: -279.0, epsilon: 0.400\n",
      "episode: 6500, score: -251.0, epsilon: 0.350\n",
      "episode: 7000, score: -218.0, epsilon: 0.300\n",
      "episode: 7500, score: -230.0, epsilon: 0.250\n",
      "episode: 8000, score: -153.0, epsilon: 0.200\n",
      "episode: 8500, score: -154.0, epsilon: 0.150\n",
      "episode: 9000, score: -214.0, epsilon: 0.100\n",
      "episode: 9500, score: -133.0, epsilon: 0.050\n",
      "episode: 10000, score: -153.0, epsilon: 0.010\n",
      "episode: 10500, score: -134.0, epsilon: 0.010\n",
      "episode: 11000, score: -172.0, epsilon: 0.010\n",
      "episode: 11500, score: -189.0, epsilon: 0.010\n",
      "episode: 12000, score: -174.0, epsilon: 0.010\n",
      "episode: 12500, score: -148.0, epsilon: 0.010\n",
      "episode: 13000, score: -136.0, epsilon: 0.010\n",
      "episode: 13500, score: -147.0, epsilon: 0.010\n",
      "episode: 14000, score: -156.0, epsilon: 0.010\n",
      "episode: 14500, score: -175.0, epsilon: 0.010\n",
      "episode: 15000, score: -149.0, epsilon: 0.010\n",
      "episode: 15500, score: -139.0, epsilon: 0.010\n",
      "episode: 16000, score: -143.0, epsilon: 0.010\n",
      "episode: 16500, score: -150.0, epsilon: 0.010\n",
      "episode: 17000, score: -123.0, epsilon: 0.010\n",
      "episode: 17500, score: -141.0, epsilon: 0.010\n",
      "episode: 18000, score: -194.0, epsilon: 0.010\n",
      "episode: 18500, score: -176.0, epsilon: 0.010\n",
      "episode: 19000, score: -120.0, epsilon: 0.010\n",
      "episode: 19500, score: -151.0, epsilon: 0.010\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1 # Learning Rate\n",
    "gamma = 0.9 # Discount Factor \n",
    "epsilon = 1 # e-Greedy \n",
    "episodes = 20000 # number of episodes\n",
    "    \n",
    "Q_rewards = Q_learning(alpha, gamma, epsilon, episodes) \n",
    "SARSA_rewards = SARSA_learning(alpha, gamma, epsilon, episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e7049",
   "metadata": {},
   "source": [
    "###### This is our run function\n",
    "It makes use of the load function to load in the saved Q-table from training so we can just run it without having to retrain\n",
    "\n",
    "It runs for 10 episodes with 200 steps, rendering the environment so you can see how it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53424de2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def run(pickle):\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env._max_episode_steps = 200\n",
    "    \n",
    "    # Load previously saved Q-table\n",
    "    Q = load_obj(pickle)\n",
    "    \n",
    "    for episode in range(10):\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        state = getState(observation)\n",
    "        # While the car don't reach the goal or number of steps < 200\n",
    "        while not done:\n",
    "            env.render()\n",
    "            # Take the best action for that state given trained values\n",
    "            action = maxAction(Q, state)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            # Go to next state\n",
    "            state = getState(observation)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe3b5a55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained-QLearning.pkl\n"
     ]
    }
   ],
   "source": [
    "run('trained-QLearning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b230d870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained-SARSA.pkl\n"
     ]
    }
   ],
   "source": [
    "run('trained-SARSA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe5fb6",
   "metadata": {},
   "source": [
    "# TO-DO: CONCLUSIONS\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
